{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ! pip install --user numpy \n",
    "# from keras.models import load_model\n",
    "# import numpy as np\n",
    "# from numpy import load\n",
    "# from numpy import expand_dims\n",
    "# from numpy import asarray\n",
    "# from numpy import savez_compressed\n",
    "# from os import listdir\n",
    "# from os.path import isdir\n",
    "# from PIL import Image\n",
    "# from mtcnn.mtcnn import MTCNN\n",
    "# import cv2\n",
    "# from numpy import load\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# from sklearn.preprocessing import Normalizer\n",
    "# from sklearn.svm import SVC\n",
    "# from random import choice\n",
    "# from numpy import load\n",
    "# from numpy import expand_dims\n",
    "# from matplotlib import pyplot\n",
    "\n",
    "# model = load_model('facenet_keras.h5')\n",
    "# # summarize input and output shape\n",
    "# print(model.inputs)\n",
    "# print(model.outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # to extract a single face from a photograph\n",
    "# def extract_face(image_input, req_size=(160,160)):\n",
    "#     # load image from file\n",
    "#     image = Image.open(image_input)\n",
    "#     # convert to RGB, if needed\n",
    "#     image = image.convert('RGB')\n",
    "#     # convert to array\n",
    "#     pixels = asarray(image)\n",
    "#     # create the detector, using default weights\n",
    "#     detector = MTCNN()\n",
    "#     # detect faces in the image\n",
    "#     results = detector.detect_faces(pixels)\n",
    "#     # extract the bounding box from the first face\n",
    "#     x1, y1, width, height = results[0]['box']\n",
    "#     # bug fix\n",
    "#     x1, y1 = abs(x1), abs(y1)\n",
    "#     x2, y2 = x1 + width, y1 + height\n",
    "#     # extract the face\n",
    "#     face = pixels[y1:y2, x1:x2]\n",
    "#     # resize pixels to the model size\n",
    "#     image = Image.fromarray(face)\n",
    "#     image = image.resize(req_size)\n",
    "#     face_array = asarray(image)\n",
    "#     return face_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'input_1:0' shape=(None, 160, 160, 3) dtype=float32>]\n",
      "[<tf.Tensor 'Bottleneck_BatchNorm/cond/Identity:0' shape=(None, 128) dtype=float32>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\saving.py:341: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    }
   ],
   "source": [
    "# ! pip install --user numpy \n",
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "from numpy import load\n",
    "from numpy import expand_dims\n",
    "from numpy import asarray\n",
    "from numpy import savez_compressed\n",
    "from os import listdir\n",
    "from os.path import isdir\n",
    "from PIL import Image\n",
    "from mtcnn.mtcnn import MTCNN\n",
    "import cv2\n",
    "from numpy import load\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.svm import SVC\n",
    "from random import choice\n",
    "from numpy import load\n",
    "from numpy import expand_dims\n",
    "from matplotlib import pyplot\n",
    "\n",
    "model = load_model('facenet_keras.h5')\n",
    "# summarize input and output shape\n",
    "print(model.inputs)\n",
    "print(model.outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to extract a single face from a photograph\n",
    "def extract_face(image_input, req_size=(160,160)):\n",
    "    # load image from file\n",
    "    image = Image.open(image_input)\n",
    "    # convert to RGB, if needed\n",
    "    image = image.convert('RGB')\n",
    "    # convert to array\n",
    "    pixels = asarray(image)\n",
    "    # create the detector, using default weights\n",
    "    detector = MTCNN()\n",
    "    # detect faces in the image\n",
    "    results = detector.detect_faces(pixels)\n",
    "    # extract the bounding box from the first face\n",
    "    x1, y1, width, height = results[0]['box']\n",
    "    # bug fix\n",
    "    x1, y1 = abs(x1), abs(y1)\n",
    "    x2, y2 = x1 + width, y1 + height\n",
    "    # extract the face\n",
    "    face = pixels[y1:y2, x1:x2]\n",
    "    # resize pixels to the model size\n",
    "    image = Image.fromarray(face)\n",
    "    image = image.resize(req_size)\n",
    "    face_array = asarray(image)\n",
    "    return face_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">loaded 14 examples for class: other\n",
      ">loaded 15 examples for class: slim_shady\n"
     ]
    }
   ],
   "source": [
    "trainX, trainy = list(), list()\n",
    "# enumerate folders, on per class\n",
    "for subdir in listdir('C:/Users/Asus/Downloads/train'):\n",
    "    faces = list()\n",
    "    # path\n",
    "    path = 'C:/Users/Asus/Downloads/train'+ '/' + subdir + '/'\n",
    "    if not isdir(path):\n",
    "        continue\n",
    "    for filename in listdir(path):\n",
    "        # path\n",
    "        file_path = path + filename\n",
    "        try:\n",
    "            face = extract_face(file_path)\n",
    "        except IndexError:\n",
    "            pass\n",
    "#         print(file_path)\n",
    "        faces.append(face)\n",
    "    labels = [subdir for _ in range(len(faces))]\n",
    "    print('>loaded %d examples for class: %s' % (len(faces), subdir))\n",
    "    # store\n",
    "    trainX.extend(faces)\n",
    "    trainy.extend(labels)\n",
    "#         faces.append(face)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">loaded 5 examples for class: other\n",
      ">loaded 7 examples for class: slim_shady\n"
     ]
    }
   ],
   "source": [
    "testX, testy = list(), list()\n",
    "# enumerate folders, on per class\n",
    "for subdir in listdir('C:/Users/Asus/Downloads/val'):\n",
    "    faces = list()\n",
    "    # path\n",
    "    path = 'C:/Users/Asus/Downloads/val'+ '/' + subdir + '/'\n",
    "    if not isdir(path):\n",
    "        continue\n",
    "    for filename in listdir(path):\n",
    "        # path\n",
    "        file_path = path + filename\n",
    "        try:\n",
    "            face = extract_face(file_path)\n",
    "        except IndexError:\n",
    "            pass\n",
    "#         print(file_path)\n",
    "        faces.append(face)\n",
    "    labels = [subdir for _ in range(len(faces))]\n",
    "    print('>loaded %d examples for class: %s' % (len(faces), subdir))\n",
    "    # store\n",
    "    testX.extend(faces)\n",
    "    testy.extend(labels)\n",
    "#         faces.append(face)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save arrays to one file in compressed format\n",
    "savez_compressed('faces-dataset.npz', trainX, trainy, testX, testy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded:  (29, 160, 160, 3) (29,) (12, 160, 160, 3) (12,)\n"
     ]
    }
   ],
   "source": [
    "data = load('faces-dataset.npz',allow_pickle=True)\n",
    "trainX, trainy, testX, testy = data['arr_0'], data['arr_1'], data['arr_2'], data['arr_3']\n",
    "print('Loaded: ', trainX.shape, trainy.shape, testX.shape, testy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Model\n"
     ]
    }
   ],
   "source": [
    "# load the facenet model\n",
    "model = load_model('facenet_keras.h5')\n",
    "print('Loaded Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the face embedding for one face\n",
    "def get_embedding(model, face_pixels):\n",
    "    # scale pixel values\n",
    "    face_pixels = face_pixels.astype('float32')\n",
    "    # standardize pixel values across channels (global)\n",
    "    mean, std = face_pixels.mean(), face_pixels.std()\n",
    "    face_pixels = (face_pixels - mean) / std\n",
    "    # transform face into one sample\n",
    "    samples = expand_dims(face_pixels, axis=0)\n",
    "    # make prediction to get embedding\n",
    "    yhat = model.predict(samples)\n",
    "    return yhat[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29, 128)\n"
     ]
    }
   ],
   "source": [
    "newTrainX = list()\n",
    "for face_pixels in trainX:\n",
    "    embedding = get_embedding(model, face_pixels)\n",
    "    newTrainX.append(embedding)\n",
    "newTrainX = asarray(newTrainX)\n",
    "print(newTrainX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12, 128)\n"
     ]
    }
   ],
   "source": [
    "# convert each face in the test set to an embedding\n",
    "newTestX = list()\n",
    "for face_pixels in testX:\n",
    "    embedding = get_embedding(model, face_pixels)\n",
    "    newTestX.append(embedding)\n",
    "newTestX = asarray(newTestX)\n",
    "print(newTestX.shape)\n",
    "# save arrays to one file in compressed format\n",
    "savez_compressed('faces-embeddings.npz', newTrainX, trainy, newTestX, testy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: train=29, test=12\n"
     ]
    }
   ],
   "source": [
    "data = load('faces-embeddings.npz')\n",
    "trainX, trainy, testX, testy = data['arr_0'], data['arr_1'], data['arr_2'], data['arr_3']\n",
    "print('Dataset: train=%d, test=%d' % (trainX.shape[0], testX.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_encoder = Normalizer(norm='l2')\n",
    "trainX = in_encoder.transform(trainX)\n",
    "testX = in_encoder.transform(testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_encoder = LabelEncoder()\n",
    "out_encoder.fit(trainy)\n",
    "trainy = out_encoder.transform(trainy)\n",
    "testy = out_encoder.transform(testy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
       "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SVC(kernel='linear')\n",
    "model.fit(trainX, trainy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename = 'linearSVC.sav'\n",
    "pickle.dump(model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: train=100.000, test=100.000\n"
     ]
    }
   ],
   "source": [
    "model = pickle.load(open(filename, 'rb'))\n",
    "yhat_train = model.predict(trainX)\n",
    "yhat_test = model.predict(testX)\n",
    "# score\n",
    "score_train = accuracy_score(trainy, yhat_train)\n",
    "score_test = accuracy_score(testy, yhat_test)\n",
    "# summarize\n",
    "print('Accuracy: train=%.3f, test=%.3f' % (score_train*100, score_test*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load('faces-dataset.npz')\n",
    "testX_faces = data['arr_2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection = choice([i for i in range(testX.shape[0])])\n",
    "random_face_pixels = testX_faces[selection]\n",
    "random_face_emb = testX[selection]\n",
    "random_face_class = testy[selection]\n",
    "random_face_name = out_encoder.inverse_transform([random_face_class])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = expand_dims(random_face_emb, axis=0)\n",
    "yhat_class = model.predict(samples)\n",
    "# yhat_prob = model.predict_proba(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_index = yhat_class[0]\n",
    "# class_probability = yhat_prob[0,class_index] * 100\n",
    "predict_names = out_encoder.inverse_transform(yhat_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: slim_shady\n",
      "Expected: slim_shady\n"
     ]
    }
   ],
   "source": [
    "print('Predicted: %s' % (predict_names[0]))  #class_probability\n",
    "print('Expected: %s' % random_face_name[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow('face',random_face_pixels)\n",
    "cv2.waitKey(5000)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# ! pip install mtcnn\n",
    "# ! pip install --upgrade --force-reinstall pillow\n",
    "\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "print(tf.__version__)\n",
    "\n",
    "# Helper libraries\n",
    "import numpy as np\n",
    "from numpy import asarray\n",
    "from PIL import Image\n",
    "\n",
    "from mtcnn.mtcnn import MTCNN\n",
    "# print version\n",
    "# print(mtcnn.__version__)\n",
    "from keras_vggface.vggface import VGGFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to extract a single face from a photograph\n",
    "def extract_face(image_input, req_size=(160,160)):\n",
    "    img = cv2.imread(image_input)\n",
    "    detector = MTCNN()\n",
    "    # detect faces in the image\n",
    "    results = detector.detect_faces(img)\n",
    "    # extract bounding box values\n",
    "    x1, y1, width, height = results[0]['box']\n",
    "    x2, y2 = x1 + width, y1 + height\n",
    "    # extract the face\n",
    "    face = img[y1:y2, x1:x2]\n",
    "    # resizing image\n",
    "    image = Image.fromarray(face)\n",
    "    image = image.resize(req_size)\n",
    "    face_array = asarray(image)\n",
    "    return face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# face = extract_face(r'C:\\Users\\Asus\\Downloads\\opencv-face-recognition\\dataset\\shady\\00000000.jpg')\n",
    "# cv2.imshow('face',face)\n",
    "# cv2.waitKey(5000)\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in_encoder = Normalizer(norm='l2')\n",
    "# model = pickle.load(open(filename, 'rb'))\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "blank_image = np.zeros((224,224,3), np.uint8)\n",
    "\n",
    "frame_width = int(cap.get(3))\n",
    "frame_height = int(cap.get(4))\n",
    "\n",
    "# out = cv2.VideoWriter('outpy.avi',cv2.VideoWriter_fourcc('M','J','P','G'), 10, (frame_width,frame_height))\n",
    "\n",
    "while(True):\n",
    "    ret, frame = cap.read()\n",
    "    detector = MTCNN()\n",
    "    results = detector.detect_faces(frame)\n",
    "    if len(results) > 0:\n",
    "        for result in results:\n",
    "            try:\n",
    "                x1, y1, width, height = result['box']\n",
    "                x2, y2 = x1 + width, y1 + height\n",
    "                cv2.rectangle(frame,(x1,y1),(x2,y2),(0,255,0),3)\n",
    "                face = frame[y1:y2,x1:x2]\n",
    "                image = np.resize(image,(160,160,3))  \n",
    "#                 cv2.imshow('frame',image)\n",
    "#                 embedding = get_embedding(model, face_pixels)\n",
    "#                 image = in_encoder.transform(embedding)\n",
    "#                 yhat_class = model.predict(image)\n",
    "#                 class_index = yhat_class[0]\n",
    "#                 predict_names = out_encoder.inverse_transform(yhat_class)\n",
    "                print(predict_names)\n",
    "#                 image = Image.fromarray(face)\n",
    "#                 image = image.resize((160,160))\n",
    "#                 face_array = asarray(image)\n",
    "#                 embedding = in_encoder.transform(embedding)\n",
    "#                 embedding = get_embedding(model, face_array)\n",
    "#                 samples = expand_dims(embedding, axis=0)\n",
    "#                 yhat_class = model.predict(samples)\n",
    "#                 predict_names = out_encoder.inverse_transform(yhat_class)\n",
    "#                 print(predict_names)\n",
    "#                 if predict_names[0] == 'slim_shady':\n",
    "#                     cv2.putText(image, 'slim_shady', (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36,255,12), 2)\n",
    "#                 else:\n",
    "#                     cv2.imshow('frame',frame)\n",
    "#                 out.write(frame)\n",
    "                cv2.imshow('frame',frame)\n",
    "            except:\n",
    "#                 cv2.imshow('face',blank_image)\n",
    "                frame = cv2.putText(frame, 'No Person', (10,10), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,0,0), 2, cv2.LINE_AA)\n",
    "                \n",
    "    else:\n",
    "#         frame = cv2.putText(frame, 'No Person', (10,10), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,0,0), 2, cv2.LINE_AA)\n",
    "        cv2.imshow('frame',frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "# When everything done, release the capture\n",
    "cap.release()\n",
    "# out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# face = extract_face('C:/Users/Asus/Downloads/data/Images\\shady_00000011.jpg')\n",
    "# cv2.imshow('face',face)\n",
    "# cv2.waitKey(5000)\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for img_name in os.listdir('Test_Images/'):\n",
    "#     img=cv2.imread(path+'/Test_Images/'+img_name)\n",
    "#     extracted_face = extract_face(img)\n",
    "#     cv2.imwrite(path+'/Test_Images/crop_img.jpg',extracted_face)\n",
    "    \n",
    "#     # Get Embeddings\n",
    "#     crop_img=load_img(path+'/Test_Images/crop_img.jpg',target_size=(224,224))\n",
    "#     crop_img=img_to_array(crop_img)\n",
    "#     crop_img=np.expand_dims(crop_img,axis=0)\n",
    "#     crop_img=preprocess_input(crop_img)\n",
    "#     img_encode=vgg_face(crop_img)\n",
    "\n",
    "#     # Make Predictions\n",
    "#     embed=K.eval(img_encode)\n",
    "#     person=classifier_model.predict(embed)\n",
    "#     name=person_rep[np.argmax(person)]\n",
    "#     os.remove(path+'/Test_Images/crop_img.jpg')\n",
    "#     cv2.rectangle(img,(left,top),(right,bottom),(0,255,0), 2)\n",
    "#     img=cv2.putText(img,name,(left,top-10),cv2.FONT_HERSHEY_SIMPLEX,1,(255,0,255),2,cv2.LINE_AA)\n",
    "#     img=cv2.putText(img,str(np.max(person)),(right,bottom+10),cv2.FONT_HERSHEY_SIMPLEX,0.5,(0,0,255),1,cv2.LINE_AA)\n",
    "#   # Save images with bounding box,name and accuracy \n",
    "#     cv2.imwrite(path+'/Predictions/'+img_name,img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.mkdir(path+'/Predictions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# person_rep={0:'owen',1:'shady'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot(img):\n",
    "#     plt.figure(figsize=(8,4))\n",
    "#     plt.imshow(img[:,:,::-1])\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_images_path=path+'/Test_Images/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load saved model\n",
    "# classifier_model=tf.keras.models.load_model('C:/Users/Asus/Downloads/face_classifier_model.h5')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save model for later use\n",
    "# tf.keras.models.save_model(classifier_model,'C:/Users/Asus/Downloads/face_classifier_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifier_model.fit(x_train,y_train,epochs=100,validation_data=(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Softmax regressor to classify images based on encoding \n",
    "# classifier_model=Sequential()\n",
    "# classifier_model.add(Dense(units=100,input_dim=x_train.shape[1],kernel_initializer='glorot_uniform'))\n",
    "# classifier_model.add(BatchNormalization())\n",
    "# classifier_model.add(Activation('tanh'))\n",
    "# classifier_model.add(Dropout(0.3))\n",
    "# classifier_model.add(Dense(units=10,kernel_initializer='glorot_uniform'))\n",
    "# classifier_model.add(BatchNormalization())\n",
    "# classifier_model.add(Activation('tanh'))\n",
    "# classifier_model.add(Dropout(0.2))\n",
    "# classifier_model.add(Dense(units=6,kernel_initializer='he_uniform'))\n",
    "# classifier_model.add(Activation('softmax'))\n",
    "# classifier_model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(),optimizer='nadam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load saved data\n",
    "# x_train=np.load('train_data.npy')\n",
    "# y_train=np.load('train_labels.npy')\n",
    "# x_test=np.load('test_data.npy')\n",
    "# y_test=np.load('test_labels.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save test and train data for later use\n",
    "# np.save('train_data',x_train)\n",
    "# np.save('train_labels',y_train)\n",
    "# np.save('test_data',x_test)\n",
    "# np.save('test_labels',y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_test=np.array(x_test)\n",
    "# y_test=np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Prepare Test Data\n",
    "# x_test=[]\n",
    "# y_test=[]\n",
    "# person_folders=os.listdir(path+'/Test_Images_crop/')\n",
    "# for i,person in enumerate(person_folders):\n",
    "#     image_names=os.listdir('Test_Images_crop/'+person+'/')\n",
    "#     for image_name in image_names:\n",
    "#         img=load_img(path+'/Test_Images_crop/'+person+'/'+image_name,target_size=(224,224))\n",
    "#         img=img_to_array(img)\n",
    "#         img=np.expand_dims(img,axis=0)\n",
    "#         img=preprocess_input(img)\n",
    "#         img_encode=vgg_face(img)\n",
    "#         x_test.append(np.squeeze(K.eval(img_encode)).tolist())\n",
    "#         y_test.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train=np.array(x_train)\n",
    "# y_train=np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# person_rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Prepare Training Data\n",
    "# x_train=[]\n",
    "# y_train=[]\n",
    "# person_folders=os.listdir(path+'/Images_crop/')\n",
    "# person_rep=dict()\n",
    "# for i,person in enumerate(person_folders):\n",
    "#     person_rep[i]=person\n",
    "#     image_names=os.listdir('Images_crop/'+person+'/')\n",
    "#     for image_name in image_names:\n",
    "#         img=load_img(path+'/Images_crop/'+person+'/'+image_name,target_size=(224,224))\n",
    "#         img=img_to_array(img)\n",
    "#         img=np.expand_dims(img,axis=0)\n",
    "#         img=preprocess_input(img)\n",
    "#         img_encode=vgg_face(img)\n",
    "#         x_train.append(np.squeeze(K.eval(img_encode)).tolist())\n",
    "#         y_train.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Remove Last Softmax layer and get model upto last flatten layer with outputs 2622 units\n",
    "# vgg_face=Model(inputs=model.layers[0].input,outputs=model.layers[-2].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load VGG Face model weights\n",
    "# model.load_weights('vgg_face_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Define VGG_FACE_MODEL architecture\n",
    "# model = Sequential()\n",
    "# model.add(ZeroPadding2D((1,1),input_shape=(224,224, 3)))\n",
    "# model.add(Convolution2D(64, (3, 3), activation='relu'))\n",
    "# model.add(ZeroPadding2D((1,1)))\n",
    "# model.add(Convolution2D(64, (3, 3), activation='relu'))\n",
    "# model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "# model.add(ZeroPadding2D((1,1)))\n",
    "# model.add(Convolution2D(128, (3, 3), activation='relu'))\n",
    "# model.add(ZeroPadding2D((1,1)))\n",
    "# model.add(Convolution2D(128, (3, 3), activation='relu'))\n",
    "# model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "# model.add(ZeroPadding2D((1,1)))\n",
    "# model.add(Convolution2D(256, (3, 3), activation='relu'))\n",
    "# model.add(ZeroPadding2D((1,1)))\n",
    "# model.add(Convolution2D(256, (3, 3), activation='relu'))\n",
    "# model.add(ZeroPadding2D((1,1)))\n",
    "# model.add(Convolution2D(256, (3, 3), activation='relu'))\n",
    "# model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "# model.add(ZeroPadding2D((1,1)))\n",
    "# model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "# model.add(ZeroPadding2D((1,1)))\n",
    "# model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "# model.add(ZeroPadding2D((1,1)))\n",
    "# model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "# model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "# model.add(ZeroPadding2D((1,1)))\n",
    "# model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "# model.add(ZeroPadding2D((1,1)))\n",
    "# model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "# model.add(ZeroPadding2D((1,1)))\n",
    "# model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "# model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "# model.add(Convolution2D(4096, (7, 7), activation='relu'))\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(Convolution2D(4096, (1, 1), activation='relu'))\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(Convolution2D(2622, (1, 1)))\n",
    "# model.add(Flatten())\n",
    "# model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for file_name in image_path_names:\n",
    "#     extracted_face = extract_face(file_name)\n",
    "#     img_path=path+'/Test_Images_crop/'+file_name.split('/')[-1].split('_')[0]+'/'+file_name.split('/')[-1]\n",
    "#     cv2.imwrite(img_path,extracted_face)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.mkdir(path+'/Test_Images_crop/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get Image names for testing\n",
    "# test_image_path_names=[]\n",
    "# for file_name in glob.glob(path+'/Images_test/*.jpg'):\n",
    "#     test_image_path_names.append(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimen = 128\n",
    "\n",
    "dir_path = r'C:\\Users\\Asus\\Downloads\\data\\images'\n",
    "out_path = r'C:\\Users\\Asus\\Downloads\\data\\processed_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_dir_list = os.listdir(dir_path)\n",
    "images = list()\n",
    "labels = list()\n",
    "# print(sub_dir_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sub_dir_list)):\n",
    "    label = i\n",
    "    image_names = os.listdir(os.path.join(dir_path ,sub_dir_list[i]))\n",
    "    for image_path in image_names:\n",
    "        path = os.path.join(dir_path ,sub_dir_list[i],image_path)\n",
    "        try :\n",
    "            image = Image.open(path)\n",
    "            resize_image = image.resize((dimen, dimen))\n",
    "            array_ = list()\n",
    "            for x in range(dimen):\n",
    "                sub_array = list()\n",
    "                for y in range(dimen):\n",
    "                    sub_array.append(resize_image.load()[x, y])\n",
    "                array_.append(sub_array)\n",
    "            image_data = np.array(array_)\n",
    "            image = np.array(np.reshape(image_data, (dimen, dimen, 3))) / 255\n",
    "            images.append(image)\n",
    "            labels.append(label)\n",
    "        except:\n",
    "            print( 'WARNING : File {} could not be processed.'.format( path ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 1, 1, 1]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 128, 3)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images = np.array( images )\n",
    "\n",
    "images[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_1 = list()\n",
    "samples_2 = list()\n",
    "labels = list()\n",
    "for i in range(6) :\n",
    "    for j in range(6) :\n",
    "        samples_1.append(images[i])\n",
    "        samples_2.append(images[j])\n",
    "        if i < 3 :\n",
    "            if j < 3 :\n",
    "                labels.append(1)\n",
    "            else:\n",
    "                labels.append(0)\n",
    "        else :\n",
    "            if j > 2 :\n",
    "                labels.append(1)\n",
    "            else:\n",
    "                labels.append(0)\n",
    "\n",
    "X1 = np.array( samples_1  )\n",
    "X2 = np.array( samples_2 )\n",
    "Y = np.array( labels )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36, 128, 128, 3)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save( '{}/x1.npy'.format( out_path ), X1 )\n",
    "np.save( '{}/x2.npy'.format( out_path ), X2 )\n",
    "np.save( '{}/y.npy'.format( out_path ) , Y )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras import models , optimizers , losses ,activations , callbacks\n",
    "from tensorflow.python.keras.layers import *\n",
    "import tensorflow.python.keras.backend as K\n",
    "from PIL import Image\n",
    "import tensorflow.keras\n",
    "import time\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Recognizer(object):\n",
    "#     def __int__(self):\n",
    "#         tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "#         self.__DIMEN = 128\n",
    "\n",
    "#         input_shape = (self.__DIMEN ** 2) * 3\n",
    "#         convolution_shape = ( self.__DIMEN , self.__DIMEN , 3 )\n",
    "#         kernel_size_1 = ( 4 , 4 )\n",
    "#         kernel_size_2 = ( 3 , 3 )\n",
    "#         pool_size_1 = ( 3 , 3 )\n",
    "#         pool_size_2 = ( 2 , 2 )\n",
    "#         strides = 1\n",
    "\n",
    "#         seq_conv_model = [\n",
    "\n",
    "#             Reshape( input_shape=input_shape , target_shape=convolution_shape),\n",
    "\n",
    "#             Conv2D( 32, kernel_size=kernel_size_1 , strides=strides , activation=activations.leaky_relu ),\n",
    "#             Conv2D( 32, kernel_size=kernel_size_1, strides=strides, activation=activations.leaky_relu),\n",
    "#             MaxPooling2D(pool_size=pool_size_1, strides=strides ),\n",
    "\n",
    "#             Conv2D( 64, kernel_size=kernel_size_2 , strides=strides , activation=activations.leaky_relu ),\n",
    "#             Conv2D( 64, kernel_size=kernel_size_2 , strides=strides , activation=activations.leaky_relu ),\n",
    "#             MaxPooling2D(pool_size=pool_size_2 , strides=strides),\n",
    "\n",
    "#             Flatten(),\n",
    "\n",
    "#             Dense( 64 , activation=activations.sigmoid )\n",
    "\n",
    "#         ]\n",
    "\n",
    "#         seq_model = tf.keras.Sequential( seq_conv_model )\n",
    "\n",
    "#         input_x1 = Input( shape=input_shape )\n",
    "#         input_x2 = Input( shape=input_shape )\n",
    "\n",
    "#         output_x1 = seq_model( input_x1 )\n",
    "#         output_x2 = seq_model( input_x2 )\n",
    "\n",
    "#         distance_euclid = Lambda( lambda tensors : K.abs( tensors[0] - tensors[1] ))( [output_x1 , output_x2] )\n",
    "#         outputs = Dense( 1 , activation=activations.sigmoid) ( distance_euclid )\n",
    "#         self.__model = models.Model( [ input_x1 , input_x2 ] , outputs )\n",
    "\n",
    "#         self.__model.compile( loss=losses.binary_crossentropy , optimizer=optimizers.Adam(lr=0.0001))\n",
    "\n",
    "#     def fit(self, X, Y ,hyperparameters):\n",
    "#         initial_time = time.time()\n",
    "#         self.__model.fit( X  , Y ,\n",
    "#                          batch_size=hyperparameters[ 'batch_size' ] ,\n",
    "#                          epochs=hyperparameters[ 'epochs' ] ,\n",
    "#                          callbacks=hyperparameters[ 'callbacks'],\n",
    "#                          validation_data=hyperparameters[ 'val_data' ]\n",
    "#                          )\n",
    "#         final_time = time.time()\n",
    "#         eta = ( final_time - initial_time )\n",
    "#         time_unit = 'seconds'\n",
    "#         if eta >= 60 :\n",
    "#             eta = eta / 60\n",
    "#             time_unit = 'minutes'\n",
    "#         self.__model.summary( )\n",
    "#         print( 'Elapsed time acquired for {} epoch(s) -> {} {}'.format( hyperparameters[ 'epochs' ] , eta , time_unit ) )\n",
    "\n",
    "#     def evaluate(self , test_X , test_Y  ) :\n",
    "#         return self.__model.evaluate(test_X, test_Y)\n",
    "\n",
    "\n",
    "#     def predict(self, X  ):\n",
    "#         predictions = self.__model.predict( X  )\n",
    "#         return predictions\n",
    "\n",
    "\n",
    "#     def summary(self):\n",
    "#         self.__model.summary()\n",
    "\n",
    "#     def save_model(self , file_path ):\n",
    "#         self.__model.save(file_path )\n",
    "\n",
    "\n",
    "#     def load_model(self , file_path ):\n",
    "#         self.__model = models.load_model(file_path)\n",
    "        \n",
    "# dimen = 128\n",
    "# input_shape = (self.__DIMEN ** 2) * 3\n",
    "# convolution_shape = ( self.__DIMEN , self.__DIMEN , 3 )\n",
    "# kernel_size_1 = (4,4)\n",
    "# kernel_size_2 = (3,3)\n",
    "# pool_size_1 = (3,3)\n",
    "# pool_size_2 = (2,2)\n",
    "# strides = 1\n",
    "\n",
    "# seq_conv_model = [\n",
    "\n",
    "#     Reshape( input_shape=input_shape , target_shape=convolution_shape),\n",
    "\n",
    "#     Conv2D(32, kernel_size=kernel_size_1 , strides=strides , activation=activations.leaky_relu),\n",
    "#     Conv2D( 32, kernel_size=kernel_size_1, strides=strides, activation=activations.leaky_relu),\n",
    "#     MaxPooling2D(pool_size=pool_size_1, strides=strides ),\n",
    "\n",
    "#     Conv2D(64, kernel_size=kernel_size_2 , strides=strides , activation=activations.leaky_relu ),\n",
    "#     Conv2D(64, kernel_size=kernel_size_2 , strides=strides , activation=activations.leaky_relu ),\n",
    "#     MaxPooling2D(pool_size=pool_size_2 , strides=strides),\n",
    "\n",
    "#     Flatten(),\n",
    "\n",
    "#     Dense( 64 , activation=activations.sigmoid )\n",
    "\n",
    "# ]\n",
    "\n",
    "# seq_model = tf.keras.Sequential( seq_conv_model )\n",
    "\n",
    "# input_x1 = Input( shape=input_shape )\n",
    "# input_x2 = Input( shape=input_shape )\n",
    "\n",
    "# output_x1 = seq_model( input_x1 )\n",
    "# output_x2 = seq_model( input_x2 )\n",
    "\n",
    "# distance_euclid = Lambda( lambda tensors : K.abs( tensors[0] - tensors[1] ))( [output_x1 , output_x2] )\n",
    "# outputs = Dense( 1 , activation=activations.sigmoid) ( distance_euclid )\n",
    "# self.__model = models.Model( [ input_x1 , input_x2 ] , outputs )\n",
    "\n",
    "# self.__model.compile( loss=losses.binary_crossentropy , optimizer=optimizers.Adam(lr=0.0001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36, 49152)\n",
      "(36, 49152)\n",
      "(36,)\n"
     ]
    }
   ],
   "source": [
    "data_dimension = 128\n",
    "\n",
    "X1 = np.load(r'C:\\Users\\Asus\\Downloads\\data\\processed_data\\x1.npy')\n",
    "X2 = np.load(r'C:\\Users\\Asus\\Downloads\\data\\processed_data\\x2.npy')\n",
    "Y = np.load(r'C:\\Users\\Asus\\Downloads\\data\\processed_data\\y.npy')\n",
    "\n",
    "X1 = X1.reshape( ( X1.shape[0]  , data_dimension**2 * 3  ) ).astype( np.float32 )\n",
    "X2 = X2.reshape( ( X2.shape[0]  , data_dimension**2 * 3  ) ).astype( np.float32 )\n",
    "\n",
    "print( X1.shape )\n",
    "print( X2.shape )\n",
    "print( Y.shape )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recognizer = Recognizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'batch_size' : 6 ,\n",
    "    'epochs' : 5 ,\n",
    "    'callbacks' : None , # [ TensorBoard( log_dir='logs/{}'.format( time.time() ) ) ] ,\n",
    "    'val_data' : None\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recognizer.fit( [ X1 , X2 ], Y, hyperparameters=parameters)\n",
    "# recognizer.save_model('models/model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'layer'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-84-851166b51956>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMaxPooling2D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpool_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpool_size_2\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstrides\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFlatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m     \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m     ])\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\sequential.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, layers, name)\u001b[0m\n\u001b[0;32m     92\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlayers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlayers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\sequential.py\u001b[0m in \u001b[0;36madd\u001b[1;34m(self, layer)\u001b[0m\n\u001b[0;32m    180\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_source_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 182\u001b[1;33m             \u001b[0moutput_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    183\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m                 raise TypeError('All layers in a Sequential model '\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36msymbolic_fn_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_SYMBOLIC_SCOPE\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mget_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[0;32m    487\u001b[0m             \u001b[1;31m# Actually call the layer,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m             \u001b[1;31m# collecting output(s), mask(s), and shape(s).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m             \u001b[0moutput_mask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprevious_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\layers\\convolutional.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivation\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 188\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    189\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    793\u001b[0m     \u001b[1;31m# framework.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    794\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mbuild_graph\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mbase_layer_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mneeds_keras_history\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 795\u001b[1;33m       \u001b[0mbase_layer_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_keras_history\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    796\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    797\u001b[0m     \u001b[1;31m# Clear eager losses on top level model call.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer_utils.py\u001b[0m in \u001b[0;36mcreate_keras_history\u001b[1;34m(tensors)\u001b[0m\n\u001b[0;32m    182\u001b[0m     \u001b[0mkeras_tensors\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mTensors\u001b[0m \u001b[0mfound\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mcame\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0ma\u001b[0m \u001b[0mKeras\u001b[0m \u001b[0mLayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m   \"\"\"\n\u001b[1;32m--> 184\u001b[1;33m   \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreated_layers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_create_keras_history_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    185\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mcreated_layers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer_utils.py\u001b[0m in \u001b[0;36m_create_keras_history_helper\u001b[1;34m(tensors, processed_ops, created_layers)\u001b[0m\n\u001b[0;32m    229\u001b[0m               \u001b[0mconstants\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbackend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    230\u001b[0m       processed_ops, created_layers = _create_keras_history_helper(\n\u001b[1;32m--> 231\u001b[1;33m           layer_inputs, processed_ops, created_layers)\n\u001b[0m\u001b[0;32m    232\u001b[0m       \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m       \u001b[0mnode_def\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSerializeToString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer_utils.py\u001b[0m in \u001b[0;36m_create_keras_history_helper\u001b[1;34m(tensors, processed_ops, created_layers)\u001b[0m\n\u001b[0;32m    236\u001b[0m       \u001b[0mcreated_layers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop_layer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    237\u001b[0m       op_layer._add_inbound_node(  # pylint: disable=protected-access\n\u001b[1;32m--> 238\u001b[1;33m           layer_inputs, op.outputs)\n\u001b[0m\u001b[0;32m    239\u001b[0m       \u001b[0mprocessed_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    240\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mprocessed_ops\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreated_layers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m_add_inbound_node\u001b[1;34m(self, input_tensors, output_tensors, arguments)\u001b[0m\n\u001b[0;32m   2052\u001b[0m     \"\"\"\n\u001b[0;32m   2053\u001b[0m     inbound_layers = nest.map_structure(lambda t: t._keras_history.layer,\n\u001b[1;32m-> 2054\u001b[1;33m                                         input_tensors)\n\u001b[0m\u001b[0;32m   2055\u001b[0m     node_indices = nest.map_structure(lambda t: t._keras_history.node_index,\n\u001b[0;32m   2056\u001b[0m                                       input_tensors)\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\util\\nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[1;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[0;32m    533\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m   return pack_sequence_as(\n\u001b[1;32m--> 535\u001b[1;33m       \u001b[0mstructure\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    536\u001b[0m       expand_composites=expand_composites)\n\u001b[0;32m    537\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\util\\nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    533\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m   return pack_sequence_as(\n\u001b[1;32m--> 535\u001b[1;33m       \u001b[0mstructure\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    536\u001b[0m       expand_composites=expand_composites)\n\u001b[0;32m    537\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m   2051\u001b[0m             \u001b[0;31m`\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0mat\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mcall\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mcreated\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2052\u001b[0m     \"\"\"\n\u001b[1;32m-> 2053\u001b[1;33m     inbound_layers = nest.map_structure(lambda t: t._keras_history.layer,\n\u001b[0m\u001b[0;32m   2054\u001b[0m                                         input_tensors)\n\u001b[0;32m   2055\u001b[0m     node_indices = nest.map_structure(lambda t: t._keras_history.node_index,\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'layer'"
     ]
    }
   ],
   "source": [
    "# Keras Sequential Model\n",
    "tf.random.set_seed(100)\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "leaky_relu = tf.keras.layers.LeakyReLU(alpha=0.3)\n",
    "\n",
    "\n",
    "dimen = 128\n",
    "input_shape = ((dimen ** 2) * 3,)\n",
    "convolution_shape = (128 , 128 , 3 )\n",
    "kernel_size_1 = (4,4)\n",
    "kernel_size_2 = (3,3)\n",
    "pool_size_1 = (3,3)\n",
    "pool_size_2 = (2,2)\n",
    "strides = 1\n",
    "\n",
    "model = None\n",
    "seq_model = keras.Sequential([\n",
    "    keras.layers.Reshape(input_shape=input_shape, target_shape = (128 , 128 , 3 )),\n",
    "    keras.layers.Conv2D( 32, kernel_size=kernel_size_1 , strides=strides , activation=leaky_relu ),\n",
    "    keras.layers.Conv2D( 32, kernel_size=kernel_size_1, strides=strides, activation=leaky_relu),\n",
    "    keras.layers.MaxPooling2D(pool_size=pool_size_1, strides=strides),\n",
    "    keras.layers.Conv2D( 64, kernel_size=kernel_size_2 , strides=strides , activation=leaky_relu ),\n",
    "    keras.layers.Conv2D( 64, kernel_size=kernel_size_2 , strides=strides , activation=leaky_relu ),\n",
    "    keras.layers.MaxPooling2D(pool_size=pool_size_2 , strides=strides),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(64)\n",
    "    ])\n",
    "    \n",
    "\n",
    "# seq_model = tf.keras.Sequential(seq_conv_model)\n",
    "\n",
    "input_x1 = Input(shape=input_shape)\n",
    "input_x2 = Input(shape=input_shape)\n",
    "\n",
    "output_x1 = seq_model(input_x1)\n",
    "output_x2 = seq_model(input_x2 )\n",
    "\n",
    "distance_euclid = Lambda( lambda tensors : K.abs( tensors[0] - tensors[1] ))( [output_x1 , output_x2] )\n",
    "outputs = Dense( 1 , activation=activations.sigmoid) ( distance_euclid )\n",
    "model = models.Model( [ input_x1 , input_x2 ] , outputs )\n",
    "\n",
    "model.compile( loss=losses.binary_crossentropy , optimizer=optimizers.Adam(lr=0.0001))\n",
    "\n",
    "initial_time = time.time()\n",
    "model.fit(X ,Y ,\n",
    "            batch_size=hyperparameters[ 'batch_size' ] ,\n",
    "            epochs=hyperparameters[ 'epochs' ] ,\n",
    "            callbacks=hyperparameters[ 'callbacks'],\n",
    "            validation_data=hyperparameters[ 'val_data' ])\n",
    "\n",
    "final_time = time.time()\n",
    "eta = ( final_time - initial_time )\n",
    "time_unit = 'seconds'\n",
    "if eta >= 60 :\n",
    "    eta = eta / 60\n",
    "    time_unit = 'minutes'\n",
    "model.summary()\n",
    "print('Elapsed time acquired for {} epoch(s) -> {} {}'.format( hyperparameters[ 'epochs' ] , eta , time_unit))\n",
    "model.save(r'C:\\Users\\Asus\\Downloads\\data\\model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_images_from_dir(dir_path , flatten=True):\n",
    "    images = list()\n",
    "    images_names = os.listdir( dir_path )\n",
    "    for imageName in images_names :\n",
    "        image = Image.open(dir_path + imageName)\n",
    "        resize_image = image.resize((dimen, dimen))\n",
    "        array = list()\n",
    "        for x in range(dimen):\n",
    "            sub_array = list()\n",
    "            for y in range(dimen):\n",
    "                sub_array.append(resize_image.load()[x, y])\n",
    "            array.append(sub_array)\n",
    "        image_data = np.array(array)\n",
    "        image = np.array(np.reshape(image_data,(dimen, dimen, 3)))\n",
    "        images.append(image)\n",
    "\n",
    "    if flatten :\n",
    "        images = np.array(images)\n",
    "        return images.reshape( ( images.shape[0], dimen**2 * 3  ) ).astype( np.float32 )\n",
    "    else:\n",
    "        return np.array(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_images = recognizer.prepare_images_from_dir( 'custom_images/'  )\n",
    "class_1_images = recognizer.prepare_images_from_dir( 'images/p1/' )\n",
    "class_2_images = recognizer.prepare_images_from_dir( 'images/p2/' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = list()\n",
    "labels = list()\n",
    "for image in custom_images:\n",
    "    label = list()\n",
    "    score = list()\n",
    "    for sample in class_1_images :\n",
    "        image , sample = image.reshape( ( 1 , -1 ) ) , sample.reshape((1 , -1 ) )\n",
    "        score.append( recognizer.predict( [ image , sample ])[0] )\n",
    "        label.append( 0 )\n",
    "    for sample in class_2_images :\n",
    "        image , sample = image.reshape( ( 1 , -1 ) ) , sample.reshape((1 , -1 ) )\n",
    "        score.append( recognizer.predict( [ image , sample ])[0] )\n",
    "        label.append( 1 )\n",
    "    labels.append( label )\n",
    "    scores.append( score )\n",
    "\n",
    "scores = np.array( scores )\n",
    "labels = np.array( labels )\n",
    "\n",
    "for i in range( custom_images.shape[0] ) :\n",
    "    index = np.argmax( scores[i] )\n",
    "    label_ = labels[i][index]\n",
    "    print( 'IMAGE {} is {} with confidence of {}'.format( i+1  , label_ , scores[i][index][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49152"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(dimen ** 2) * 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_images_from_dir(dir_path , flatten=True) :\n",
    "    images = list()\n",
    "    folder_list = ['Shady', 'Unknown', 'sample']\n",
    "    images_names = os.listdir( dir_path )\n",
    "    for imageName in images_names :\n",
    "        if imageName in images_names:\n",
    "            image = Image.open(dir_path + imageName)\n",
    "            resize_image = image.resize((128, 128))\n",
    "            array = list()\n",
    "            for x in range(128):\n",
    "                sub_array = list()\n",
    "            for y in range(128):\n",
    "                sub_array.append(resize_image.load()[x, y])\n",
    "            array.append(sub_array)\n",
    "            image_data = np.array(array)\n",
    "            image = np.array(np.reshape(image_data,(128, 128, 3)))\n",
    "            images.append(image)\n",
    "        \n",
    "    if flatten :\n",
    "        images = np.array(images)\n",
    "        return images.reshape( ( images.shape[0]  , (128**2) * 3  ) ).astype( np.float32 )\n",
    "    else:\n",
    "        return np.array(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
